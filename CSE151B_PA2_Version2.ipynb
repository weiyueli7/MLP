{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7737409",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6cad3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import copy\n",
    "import numpy as np\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def one_hot_encoding(labels, num_classes=10):\n",
    "    \"\"\"\n",
    "    Encode labels using one hot encoding and return them.\n",
    "    \"\"\"\n",
    "    return np.eye(num_classes)[labels]\n",
    "\n",
    "\n",
    "def onehot_decode(labels):\n",
    "    \"\"\"\n",
    "    Performs one-hot decoding on labels.\n",
    "\n",
    "    Ideas:\n",
    "        NumPy's `argmax` function \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    labels : np.array\n",
    "        2d array (shape n*k) with each row corresponding to \n",
    "        a one-hot encoded version of the original value.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        1d array (length n) of targets (k)\n",
    "    \"\"\"\n",
    "    # return the onehot decoded vector by using given `argmax` hint\n",
    "    return np.argmax(labels, axis=1)\n",
    "\n",
    "\n",
    "def write_to_file(path, data):\n",
    "    \"\"\"\n",
    "    Dumps pickled data into the specified relative path.\n",
    "\n",
    "    Args:\n",
    "        path: relative path to store to\n",
    "        data: data to pickle and store\n",
    "    \"\"\"\n",
    "    with open(path, 'wb') as handle:\n",
    "        pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def load_data(train=True):\n",
    "    \"\"\"\n",
    "    Load the data from disk\n",
    "\n",
    "    Args:\n",
    "        train: Load training data if true, else load test data\n",
    "\n",
    "    Returns:\n",
    "        Tuple:\n",
    "            Images\n",
    "            Labels\n",
    "    \"\"\"\n",
    "    directory = 'train' if train else 'test'\n",
    "    patterns = np.load(os.path.join('./data/', directory, 'images.npz'))['arr_0']\n",
    "    labels = np.load(os.path.join('./data/', directory, 'labels.npz'))['arr_0']\n",
    "    return patterns.reshape(len(patterns), -1), labels\n",
    "\n",
    "\n",
    "def load_config(path):\n",
    "    \"\"\"\n",
    "    Load the configuration from config.yaml\n",
    "\n",
    "    Args:\n",
    "        path: A relative path to the config.yaml file\n",
    "\n",
    "    Returns:\n",
    "        A dict object containing the parameters specified in the config file\n",
    "    \"\"\"\n",
    "    return yaml.load(open(path, 'r'), Loader=yaml.SafeLoader)\n",
    "\n",
    "\n",
    "def generate_k_fold_set(dataset, k=5):\n",
    "    \"\"\"\n",
    "    Creates a generator object to generate k folds for k fold cross validation.\n",
    "\n",
    "    Args:\n",
    "        dataset: The dataset to create folds on\n",
    "        k: The number of folds\n",
    "\n",
    "    Returns:\n",
    "        A train and validation fold for each call, up to k times\n",
    "    \"\"\"\n",
    "    X, y = dataset\n",
    "    if k == 1:\n",
    "        yield (X, y), (X[len(X):], y[len(y):])\n",
    "        return\n",
    "\n",
    "    order = np.random.permutation(len(X))\n",
    "\n",
    "    fold_width = len(X) // k\n",
    "\n",
    "    l_idx, r_idx = 0, fold_width\n",
    "\n",
    "    for i in range(k):\n",
    "        train = np.concatenate([X[order[:l_idx]], X[order[r_idx:]]]), np.concatenate(\n",
    "            [y[order[:l_idx]], y[order[r_idx:]]])\n",
    "        validation = X[order[l_idx:r_idx]], y[order[l_idx:r_idx]]\n",
    "        yield train, validation\n",
    "        l_idx, r_idx = r_idx, r_idx + fold_width\n",
    "\n",
    "\n",
    "def z_score_normalize(X, u=None, sd=None):\n",
    "    \"\"\"\n",
    "    Performs z-score normalization on X.\n",
    "    f(x) = (x - μ) / σ\n",
    "        where\n",
    "            μ = mean of x\n",
    "            σ = standard deviation of x\n",
    "\n",
    "    Args:\n",
    "        X: the data to min-max normalize\n",
    "        u: the mean to normalize X with\n",
    "        sd: the standard deviation to normalize X with\n",
    "\n",
    "    Returns:\n",
    "        Tuple:\n",
    "            Transformed dataset with mean 0 and stdev 1\n",
    "            Computed statistics (mean and stdev) for the dataset to undo z-scoring.\n",
    "\n",
    "    \"\"\"\n",
    "    if u is None:\n",
    "        u = np.mean(X, axis=0)\n",
    "    if sd is None:\n",
    "        sd = np.std(X, axis=0)\n",
    "    return ((X - u) / sd), (u, sd)\n",
    "\n",
    "\n",
    "def shuffle(dataset):\n",
    "    \"\"\"\n",
    "    Shuffle dataset.\n",
    "\n",
    "    Make sure that corresponding images and labels are kept together. \n",
    "    Ideas: \n",
    "        NumPy array indexing \n",
    "            https://numpy.org/doc/stable/user/basics.indexing.html#advanced-indexing\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset\n",
    "        Tuple containing\n",
    "            Images (X)\n",
    "            Labels (y)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        Tuple containing\n",
    "            Images (X)\n",
    "            Labels (y)\n",
    "    \"\"\"\n",
    "    # find the number of images in the data and save their indexes as an array\n",
    "    idx = np.arange(len(dataset[0]))\n",
    "    # shuffle the above array in-place\n",
    "    np.random.shuffle(idx)\n",
    "    # return the shuffled dataset\n",
    "    return dataset[0][idx], dataset[1][idx]\n",
    "\n",
    "\n",
    "def generate_minibatches(dataset, batch_size=128):\n",
    "    \"\"\"\n",
    "    Helper method to generate minibatches\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset\n",
    "        Tuple containing\n",
    "            Images (X)\n",
    "            Labels (y)\n",
    "    batch_size\n",
    "        int with default of 64\n",
    "    Returns\n",
    "    -------\n",
    "        smaller Tuple:\n",
    "            Images (X)\n",
    "            Labels (y)\n",
    "    \"\"\"\n",
    "    # given by the startercode without modification\n",
    "    X, y = dataset\n",
    "    l_idx, r_idx = 0, batch_size\n",
    "    while r_idx < len(X):\n",
    "        yield X[l_idx:r_idx], y[l_idx:r_idx]\n",
    "        l_idx, r_idx = r_idx, r_idx + batch_size\n",
    "    yield X[l_idx:], y[l_idx:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d452f292",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fa7733",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "class Activation:\n",
    "    \"\"\"\n",
    "    The class implements different types of activation functions for\n",
    "    your neural network layers.\n",
    "\n",
    "    Example (for sigmoid):\n",
    "        >>> sigmoid_layer = Activation(\"sigmoid\")\n",
    "        >>> z = sigmoid_layer(a)\n",
    "        >>> gradient = sigmoid_layer.backward(delta=1.0)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, activation_type=\"sigmoid\"):\n",
    "        \"\"\"\n",
    "        Initialize activation type and placeholders here.\n",
    "        \"\"\"\n",
    "        if activation_type not in [\"sigmoid\", \"tanh\", \"ReLU\"]:\n",
    "            raise NotImplementedError(\"%s is not implemented.\" % (activation_type))\n",
    "\n",
    "        # Type of non-linear activation.\n",
    "        self.activation_type = activation_type\n",
    "        # Placeholder for input. This will be used for computing gradients.\n",
    "        self.x = None\n",
    "\n",
    "    def __call__(self, a):\n",
    "        \"\"\"\n",
    "        This method allows your instances to be callable.\n",
    "        \"\"\"\n",
    "        return self.forward(a)\n",
    "\n",
    "    def forward(self, a):\n",
    "        \"\"\"\n",
    "        Compute the forward pass.\n",
    "        \"\"\"\n",
    "        self.x = a\n",
    "        if self.activation_type == \"sigmoid\":\n",
    "            return self.sigmoid(a)\n",
    "\n",
    "        elif self.activation_type == \"tanh\":\n",
    "            return self.tanh(a)\n",
    "\n",
    "        elif self.activation_type == \"ReLU\":\n",
    "            return self.ReLU(a)\n",
    "\n",
    "    def backward(self, delta):\n",
    "        \"\"\"\n",
    "        Compute the backward pass.\n",
    "        \"\"\"\n",
    "        if self.activation_type == \"sigmoid\":\n",
    "            grad = self.grad_sigmoid()\n",
    "\n",
    "        elif self.activation_type == \"tanh\":\n",
    "            grad = self.grad_tanh()\n",
    "\n",
    "        elif self.activation_type == \"ReLU\":\n",
    "            grad = self.grad_ReLU()\n",
    "\n",
    "        return grad * delta\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        Implement the sigmoid activation here.\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-1 * x))\n",
    "\n",
    "    def tanh(self, x):\n",
    "        \"\"\"\n",
    "        Implement tanh here.\n",
    "        \"\"\"\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def ReLU(self, x):\n",
    "        \"\"\"\n",
    "        Implement ReLU here.\n",
    "        \"\"\"\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def grad_sigmoid(self):\n",
    "        \"\"\"\n",
    "        Compute the gradient for sigmoid here.\n",
    "        \"\"\"\n",
    "        return self.sigmoid(self.x) * (1 - self.sigmoid(self.x))\n",
    "\n",
    "    def grad_tanh(self):\n",
    "        \"\"\"\n",
    "        Compute the gradient for tanh here.\n",
    "        \"\"\"\n",
    "        return 1 - (self.tanh(self.x)) ** 2\n",
    "\n",
    "    def grad_ReLU(self):\n",
    "        \"\"\"\n",
    "        Compute the gradient for ReLU here.\n",
    "        \"\"\"\n",
    "        return (self.x > 0) * 1\n",
    "\n",
    "\n",
    "class Layer:\n",
    "    \"\"\"\n",
    "    This class implements Fully Connected layers for your neural network.\n",
    "\n",
    "    Example:\n",
    "        >>> fully_connected_layer = Layer(1024, 100)\n",
    "        >>> output = fully_connected_layer(input)\n",
    "        >>> gradient = fully_connected_layer.backward(delta)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_units, out_units, config):\n",
    "        \"\"\"\n",
    "        Define the architecture and create placeholder.\n",
    "        \"\"\"\n",
    "        np.random.seed(42)\n",
    "        self.config = config\n",
    "        self.w = math.sqrt(2 / in_units) * np.random.randn(in_units,\n",
    "                                                           out_units)  # You can experiment with initialization.\n",
    "        self.b = np.zeros((1, out_units))  # Create a placeholder for Bias\n",
    "        self.x = None  # Save the input to forward in this\n",
    "        self.a = None  # Save the output of forward pass in this (without activation)\n",
    "\n",
    "        self.d_x = None  # Save the gradient w.r.t x in this\n",
    "        self.d_w = None  # Save the gradient w.r.t w in this\n",
    "        self.d_b = None  # Save the gradient w.r.t b in this\n",
    "        self.m_w = 0\n",
    "        self.m_b = 0\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Make layer callable.\n",
    "        \"\"\"\n",
    "        return self.forward(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Compute the forward pass through the layer here.\n",
    "        Do not apply activation here.\n",
    "        Return self.a\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        self.a = self.x @ self.w + self.b\n",
    "        return self.a\n",
    "\n",
    "    def backward(self, delta, gamma):\n",
    "        \"\"\"\n",
    "        Write the code for backward pass. This takes in gradient from its next layer as input,\n",
    "        computes gradient for its weights and the delta to pass to its previous layers.\n",
    "        Return self.dx\n",
    "        \"\"\"\n",
    "        prev_delta = delta @ self.w.T\n",
    "        self.d_x = delta @ self.w.T\n",
    "        self.d_w = -(self.x.T @ delta)\n",
    "        self.d_b = -np.sum(delta, axis = 0)\n",
    "        self.m_w = gamma * self.m_w + (1 - gamma) * self.d_w\n",
    "        self.m_b = gamma * self.m_b + (1 - gamma) * self.d_b\n",
    "        if (self.config['momentum']):                   \n",
    "            self.w = self.w - self.config['learning_rate'] * self.m_w / self.config['batch_size']\n",
    "            self.b = self.b - self.config['learning_rate'] * self.m_b / self.config['batch_size']\n",
    "        else:\n",
    "            self.w = self.w - self.config['learning_rate'] * self.d_w / self.config['batch_size']\n",
    "            self.b = self.b - self.config['learning_rate'] * self.d_b / self.config['batch_size'] \n",
    "        return prev_delta\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    Create a Neural Network specified by the input configuration.\n",
    "\n",
    "    Example:\n",
    "        >>> net = NeuralNetwork(config)\n",
    "        >>> output = net(input)\n",
    "        >>> net.backward()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        Create the Neural Network using config.\n",
    "        \"\"\"\n",
    "        self.layers = []  # Store all layers in this list.\n",
    "        self.x = None  # Save the input to forward in this\n",
    "        self.y = None  # Save the output vector of model in this\n",
    "        self.targets = None  # Save the targets in forward in this variable\n",
    "        self.deltas = []\n",
    "        self.config = config\n",
    "\n",
    "        # Add layers specified by layer_specs.\n",
    "        for i in range(len(self.config['layer_specs']) - 1):\n",
    "            self.layers.append(Layer(self.config['layer_specs'][i], self.config['layer_specs'][i + 1], self.config))\n",
    "            if i < len(self.config['layer_specs']) - 2:\n",
    "                self.layers.append(Activation(self.config['activation']))\n",
    "\n",
    "    def __call__(self, x, targets=None):\n",
    "        \"\"\"\n",
    "        Make NeuralNetwork callable.\n",
    "        \"\"\"\n",
    "        return self.forward(x, targets)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        \"\"\"\n",
    "        Compute forward pass through all the layers in the network and return it.\n",
    "        If targets are provided, return loss as well.\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        self.targets = targets\n",
    "        for layer in self.layers:\n",
    "            self.x = layer(self.x)\n",
    "        self.y = self.softmax(self.x)\n",
    "        if targets is None:\n",
    "            return self.y, None\n",
    "        return self.y, self.loss(self.y, self.targets)\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Implement backpropagation here.\n",
    "        Call backward methods of individual layer's.\n",
    "        \"\"\"\n",
    "        delta = self.targets - self.y\n",
    "        self.deltas = [delta]\n",
    "        for i in range(len(self.layers) - 1, -1, -1):\n",
    "            if isinstance(self.layers[i], Layer):\n",
    "                delta = self.layers[i].backward(delta, self.config['momentum_gamma'])\n",
    "            else:\n",
    "                delta = self.layers[i].backward(delta)\n",
    "            self.deltas.append(delta)\n",
    "        return delta\n",
    "\n",
    "    def softmax(self, x):\n",
    "        \"\"\"\n",
    "        Implement the softmax function here.\n",
    "        Remember to take care of the overflow condition.\n",
    "        \"\"\"\n",
    "        return np.exp(x - np.max(x, axis = 1, keepdims = True)) \\\n",
    "               / np.sum(np.exp(x - np.max(x, axis = 1, keepdims = True)), axis = 1).reshape(-1,1) \n",
    "\n",
    "    def loss(self, logits, targets):\n",
    "        \"\"\"\n",
    "        compute the categorical cross-entropy loss and return it.\n",
    "        \"\"\"\n",
    "        return -np.sum(targets * np.log(logits + 1e-20)) / len(targets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2b8b1b",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504f2e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import write_to_file\n",
    "#from neuralnet import *\n",
    "\n",
    "def train(x_train, y_train, x_val, y_val, config):\n",
    "    \"\"\"\n",
    "    Train your model here using batch stochastic gradient descent and early stopping. Use config to set parameters\n",
    "    for training like learning rate, momentum, etc.\n",
    "\n",
    "    Args:\n",
    "        x_train: The train patterns\n",
    "        y_train: The train labels\n",
    "        x_val: The validation set patterns\n",
    "        y_val: The validation set labels\n",
    "        config: The configs as specified in config.yaml\n",
    "        experiment: An optional dict parameter for you to specify which experiment you want to run in train.\n",
    "\n",
    "    Returns:\n",
    "        5 things:\n",
    "            training and validation loss and accuracies - 1D arrays of loss and accuracy values per epoch.\n",
    "            best model - an instance of class NeuralNetwork. You can use copy.deepcopy(model) to save the best model.\n",
    "    \"\"\"\n",
    "    train_acc = []\n",
    "    val_acc = []\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    best_model = None\n",
    "    prev_loss = float(\"inf\")\n",
    "    best_model_loss = float(\"inf\")\n",
    "    \n",
    "    model = NeuralNetwork(config=config)\n",
    "    \n",
    "    patience = 0\n",
    "    \n",
    "    count_epoch = 0\n",
    "    \n",
    "    for epoch in tqdm(range(config['epochs'])):\n",
    "        \n",
    "        count_epoch += 1\n",
    "        \n",
    "        x_train, y_train = shuffle((x_train, y_train))\n",
    "        \n",
    "        x_val, y_val = shuffle((x_val, y_val))\n",
    "        \n",
    "        for batch in generate_minibatches((x_train, y_train), config['batch_size']):\n",
    "            \n",
    "            pred, loss = model.forward(batch[0], batch[1])\n",
    "            \n",
    "            model.backward()                       \n",
    "        \n",
    "        train_pred, train_losses = model.forward(x_train, y_train)\n",
    "        \n",
    "        train_accuracy = accuracy(train_pred, y_train)\n",
    "        \n",
    "        train_loss.append(train_losses)\n",
    "        \n",
    "        train_acc.append(train_accuracy)\n",
    "        \n",
    "        val_pred, val_losses = model.forward(x_val, y_val)\n",
    "        \n",
    "        val_accuracy = accuracy(val_pred, y_val)\n",
    "\n",
    "        val_loss.append(val_losses)\n",
    "        \n",
    "        val_acc.append(val_accuracy)\n",
    "        \n",
    "        if val_losses < best_model_loss:\n",
    "            \n",
    "            best_model_loss = val_losses\n",
    "            \n",
    "            best_model = copy.deepcopy(model)\n",
    "            \n",
    "        if config['early_stop']:\n",
    "        \n",
    "            if val_losses > prev_loss:\n",
    "            \n",
    "                patience += 1\n",
    "            \n",
    "                if patience == 5:\n",
    "                \n",
    "                    break\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                patience = 0\n",
    "        \n",
    "        prev_loss = val_losses\n",
    "        \n",
    "    return train_acc, val_acc, train_loss, val_loss, best_model                 \n",
    "    \n",
    "\n",
    "\n",
    "def test(model, x_test, y_test):\n",
    "    \"\"\"\n",
    "    Does a forward pass on the model and returns loss and accuracy on the test set.\n",
    "\n",
    "    Args:\n",
    "        model: The trained model to run a forward pass on.\n",
    "        x_test: The test patterns.\n",
    "        y_test: The test labels.\n",
    "\n",
    "    Returns:\n",
    "        Loss, Test accuracy\n",
    "    \"\"\"\n",
    "    # return loss, accuracy\n",
    "    test_pred, test_loss = model.forward(x_test, y_test)\n",
    "    \n",
    "    acc = np.mean(np.argmax(test_pred, axis = 1) == onehot_decode(y_test))\n",
    "    \n",
    "    return test_loss, acc\n",
    "\n",
    "def accuracy(pred, target):\n",
    "    return np.mean(np.argmax(pred, axis = 1) == onehot_decode(target))\n",
    "\n",
    "\n",
    "def train_mlp(x_train, y_train, x_val, y_val, x_test, y_test, config):\n",
    "    \"\"\"\n",
    "    This function trains a single multi-layer perceptron and plots its performances.\n",
    "\n",
    "    NOTE: For this function and any of the experiments, feel free to come up with your own ways of saving data\n",
    "            (i.e. plots, performances, etc.). A recommendation is to save this function's data and each experiment's\n",
    "            data into separate folders, but this part is up to you.\n",
    "    \"\"\"\n",
    "    # train the model\n",
    "    train_acc, valid_acc, train_loss, valid_loss, best_model = \\\n",
    "        train(x_train, y_train, x_val, y_val, config)\n",
    "\n",
    "    test_loss, test_acc = test(best_model, x_test, y_test)\n",
    "\n",
    "    print(\"Config: %r\" % config)\n",
    "    print(\"Train Loss: \", train_loss[-1])\n",
    "    print(\"Validation Loss: \", valid_loss[-1])\n",
    "    print(\"Validation Accuracy: \", valid_acc[-1])\n",
    "    print(\"Test Loss: \", test_loss)\n",
    "    print(\"Test Accuracy: \", test_acc)\n",
    "\n",
    "    # DO NOT modify the code below.\n",
    "    data = {'train_loss': train_loss, 'val_loss': valid_loss, 'train_acc': train_acc, 'val_acc': valid_acc,\n",
    "            'best_model': best_model, 'test_loss': test_loss, 'test_acc': test_acc}\n",
    "\n",
    "    #write_to_file('./results.pkl', data)\n",
    "\n",
    "\n",
    "def activation_experiment(x_train, y_train, x_val, y_val, x_test, y_test, config):\n",
    "    \"\"\"\n",
    "    This function tests all the different activation functions available and then plots their performances.\n",
    "    \"\"\"\n",
    "    activations = ['sigmoid', 'tanh', 'ReLU']\n",
    "    config_copy = config\n",
    "    for activation in activations:\n",
    "        config_copy['activation'] = activation\n",
    "        train_mlp(x_train, y_train, x_val, y_val, x_test, y_test, config_copy)\n",
    "\n",
    "def topology_experiment(x_train, y_train, x_val, y_val, x_test, y_test, config):\n",
    "    \"\"\"\n",
    "    This function tests performance of various network topologies, i.e. making\n",
    "    the graph narrower and wider by halving and doubling the number of hidden units.\n",
    "\n",
    "    Then, we change number of hidden layers to 2 of equal size instead of 1, and keep\n",
    "    number of parameters roughly equal to the number of parameters of the best performing\n",
    "    model previously.\n",
    "    \"\"\"\n",
    "    units = [64, 128, 256]\n",
    "    config_copy = config\n",
    "    for unit in units:\n",
    "        config_copy['layer_specs'][1] = unit\n",
    "        train_mlp(x_train, y_train, x_val, y_val, x_test, y_test, config_copy)\n",
    "    double_hidden = [784, 112, 112, 10]\n",
    "    config_copy['layer_specs'] = double_hidden\n",
    "    train_mlp(x_train, y_train, x_val, y_val, x_test, y_test, config_copy)\n",
    "\n",
    "\n",
    "def regularization_experiment(x_train, y_train, x_val, y_val, x_test, y_test, config):\n",
    "    \"\"\"\n",
    "    This function tests the neural network with regularization.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError('Regularization Experiment not implemented')\n",
    "\n",
    "\n",
    "def check_gradients(x_train, y_train, adjust, config):\n",
    "    \"\"\"\n",
    "    Check the network gradients computed by back propagation by comparing with the gradients computed using numerical\n",
    "    approximation.\n",
    "    \"\"\"\n",
    "    model = NeuralNetwork(load_config('config.yaml'))\n",
    "    layer = model.layers[0]\n",
    "    save = copy.deepcopy(layer.w[0][0])\n",
    "    \n",
    "    layer.w[0][0] += adjust\n",
    "    loss_one = model(x_train, y_train)[1]\n",
    "    \n",
    "    layer.w[0][0] = save\n",
    "    layer.w[0][0] -= adjust\n",
    "    loss_two = model(x_train, y_train)[1]\n",
    "    \n",
    "    numeric = (loss_one - loss_two) / (2 * adjust)\n",
    "    \n",
    "    layer.w[0][0] = save\n",
    "    model(x_train, y_train)\n",
    "    model.backward()\n",
    "    backward_result = layer.d_w[0][0]\n",
    "    \n",
    "    diff = abs(backward_result - numeric)\n",
    "    return diff\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6ac218",
   "metadata": {},
   "source": [
    "# Training and Testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa31f809",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = z_score_normalize(load_data(True)[0])[0]\n",
    "y_train = one_hot_encoding(load_data(True)[1], num_classes=10)\n",
    "X_test = z_score_normalize(load_data(False)[0])[0]\n",
    "y_test = one_hot_encoding(load_data(False)[1], num_classes=10)\n",
    "X_train, y_train = shuffle((X_train, y_train))\n",
    "X_test, y_test = shuffle((X_test, y_test))\n",
    "ind = int(0.8 * len(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a4e53c",
   "metadata": {},
   "source": [
    "# Activation_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cacea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_experiment(X_train[0:ind], y_train[0:ind], X_train[ind:], \n",
    "                      y_train[ind:], X_test, y_test, load_config('config.yaml'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4097fb51",
   "metadata": {},
   "source": [
    "# Topology_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006e8c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "topology_experiment(X_train[0:ind], y_train[0:ind], X_train[ind:], \n",
    "                      y_train[ind:], X_test, y_test, load_config('config.yaml'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b8903f",
   "metadata": {},
   "source": [
    "# Check_gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4a9826",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "# 700 和 800 可换成任意相距100的两个数字\n",
    "min_lst = []\n",
    "for i in tqdm(range(0, 10001, 1)):\n",
    "    if check_gradients(np.array([X_train[i]]), np.array([y_train[i]]), 1e-2, load_config('config.yaml')) < 1e-3:\n",
    "        min_lst.append(check_gradients(np.array([X_train[i]]), np.array([y_train[i]]), 1e-2, load_config('config.yaml')))\n",
    "        count += 1\n",
    "print(str(count) + \" out of 10000 satisfied the boundry!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47194354",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b761272",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
